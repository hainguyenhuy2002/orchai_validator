{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labeling.upload import ETLProcessor, get_spark, get_batch_intervals, sampling\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.load(\"config/local_full.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/01/18 14:43:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/01/18 14:43:16 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = get_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_intervals(start_block, end_block, batch_size, vote_proposed_win_size, label_win_size, min_block):\n",
    "    intervals = []\n",
    "    while start_block <= end_block:\n",
    "        batch_start = max(min_block, start_block - vote_proposed_win_size + 1)\n",
    "        batch_end = batch_start + batch_size - 1\n",
    "        \n",
    "        if batch_end > end_block:\n",
    "            batch_size = end_block - batch_start + 1\n",
    "            batch_size = (batch_size - vote_proposed_win_size + 1) // combine_win_size * combine_win_size + vote_proposed_win_size - 1\n",
    "            batch_end = batch_start + batch_size - 1\n",
    "            \n",
    "            if batch_start > batch_end:\n",
    "                break\n",
    "            \n",
    "            if batch_size < vote_proposed_win_size + combine_win_size + label_win_size - 1:\n",
    "                break\n",
    "\n",
    "        intervals.append((batch_start, batch_end))\n",
    "        start_block = batch_end - label_win_size + 1\n",
    "    \n",
    "    return intervals\n",
    "\n",
    "\n",
    "vote_proposed_win_size = config.hp.etl.vote_proposed_win_size\n",
    "label_win_size = config.hp.etl.label_win_size\n",
    "batch_size = config.hp.upload.batch_size\n",
    "\n",
    "for bs, be in get_batch_intervals(\n",
    "    start_block=7063372,\n",
    "    end_block=7065370,\n",
    "    batch_size=batch_size,\n",
    "    vote_proposed_win_size=vote_proposed_win_size,\n",
    "    label_win_size=label_win_size,\n",
    "    min_block=7049472\n",
    "):\n",
    "    print(bs, be)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7064871 - 7062873 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully queried data from database\n"
     ]
    }
   ],
   "source": [
    "df = sampling(spark, config, 7062873, 7064871)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy(\"block_height\").count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Successfully converted voting_power_score column\n",
      "------------------------------------------------\n",
      "Successfully converted commission_score column\n",
      "------------------------------------------------\n",
      "Successfully converted self_bonded_score column\n",
      "------------------------------------------------\n",
      "Successfully converted vote_propose_score column\n",
      "------------------------------------------------\n",
      "Sucessfully converted final_score\n",
      "------------------------------------------------\n",
      "Sucessfully shifting data\n",
      "------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dff = ETLProcessor.data_scoring(df, **config.hp.etl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff.groupBy(\"block_height\").count().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = dff.groupBy(\"block_height\").count().pandas_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dp.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh = d[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "cnt = Counter(np.diff(bh))\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bh[0], bh[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7063372, 7064792\n",
    "7062873, 7064871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7063372 - 7062873 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7064871 - 7064792 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "953727d1e95b678a3946b8bfc3d5a175ea50e205ad779370d1fb67fabbb85d7d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
