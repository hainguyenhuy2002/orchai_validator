{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator\n"
     ]
    }
   ],
   "source": [
    "cd ..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from labeling.tools import *\n",
    "from labeling.etl_processor import *\n",
    "from labeling.upload import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/02/01 09:32:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from labeling.back_test import *\n",
    "from omegaconf import OmegaConf\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = get_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/percent.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7118573"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7103573+  15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator/data/backtest_data already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator/notebooks/07_backtest.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator/notebooks/07_backtest.ipynb#ch0000008?line=0'>1</a>\u001b[0m back_test(path, \u001b[39m0.1\u001b[39;49m, \u001b[39m7103573\u001b[39;49m, \u001b[39m7253573\u001b[39;49m, \u001b[39m432000\u001b[39;49m)\n",
      "File \u001b[0;32m~/20221/lab_blockchain/orchai_validator/labeling/back_test.py:92\u001b[0m, in \u001b[0;36mback_test\u001b[0;34m(path, C_R_BASE, start_block, end_block, timestamp_block)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator/labeling/back_test.py?line=88'>89</a>\u001b[0m         merge_df \u001b[39m=\u001b[39m join_df(real_APR_df, unreal_APR_df)\n\u001b[1;32m     <a href='file:///Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator/labeling/back_test.py?line=89'>90</a>\u001b[0m         final_df \u001b[39m=\u001b[39m final_df\u001b[39m.\u001b[39munion(merge_df)\n\u001b[0;32m---> <a href='file:///Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator/labeling/back_test.py?line=91'>92</a>\u001b[0m final_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mparquet(\u001b[39m\"\u001b[39;49m\u001b[39mdata/backtest_data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py:1250\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=1247'>1248</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpartitionBy(partitionBy)\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=1248'>1249</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m-> <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/readwriter.py?line=1249'>1250</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mparquet(path)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py:1309\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1302'>1303</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1303'>1304</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1304'>1305</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1305'>1306</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1307'>1308</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1308'>1309</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1309'>1310</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1311'>1312</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/py4j/java_gateway.py?line=1312'>1313</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py:117\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=112'>113</a>\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=114'>115</a>\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=115'>116</a>\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=116'>117</a>\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=117'>118</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/nguyenhuyhai/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/utils.py?line=118'>119</a>\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/Users/nguyenhuyhai/20221/lab_blockchain/orchai_validator/data/backtest_data already exists."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/02/01 10:10:17 ERROR AsyncEventQueue: Dropping event from queue appStatus. This likely means one of the listeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.\n",
      "23/02/01 10:10:17 WARN AsyncEventQueue: Dropped 1 events from appStatus since the application started.\n",
      "[Stage 492:(190 + 9) / 200][Stage 493:(5 + 11) / 200][Stage 494:(14 + -1) / 200]\r"
     ]
    }
   ],
   "source": [
    "back_test(path, 0.1, 7103573, 7253573, 432000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('data/backtest_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+------------------+\n",
      "|block_num|          real_APR|        unreal_APR|\n",
      "+---------+------------------+------------------+\n",
      "|  7105073|1.0623155365538328|1.0382079618559608|\n",
      "|  7104923|1.0623107526108952| 1.038207961855961|\n",
      "|  7104623|1.0623005659830278| 1.038207961855961|\n",
      "|  7104323| 1.062290049395952|1.0381331479882263|\n",
      "|  7103873|1.0622741837859921| 1.038133147988226|\n",
      "|  7103723|1.0622689052117624|1.0381331479882263|\n",
      "|  7103573|  1.06226362150856|1.0381331479882263|\n",
      "|  7104023|1.0622794575343104|1.0381331479882265|\n",
      "|  7104773|1.0623057922525851|1.0382079618559608|\n",
      "|  7104473|1.0622953108007185|1.0382079618559605|\n",
      "|  7104173|1.0622847852191046| 1.038133147988226|\n",
      "+---------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c69d1bf7a53832cd0f08d72949e75e194b0bf514d44a584700757793760547d4"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
