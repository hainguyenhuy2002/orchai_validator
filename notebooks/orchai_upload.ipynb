{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:48:46.277528Z","iopub.status.busy":"2022-12-06T01:48:46.276955Z","iopub.status.idle":"2022-12-06T01:48:53.729073Z","shell.execute_reply":"2022-12-06T01:48:53.727578Z","shell.execute_reply.started":"2022-12-06T01:48:46.277392Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2022-12-06 01:48:47--  https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n","Resolving dlcdn.apache.org (dlcdn.apache.org)... 151.101.2.132, 2a04:4e42::644\n","Connecting to dlcdn.apache.org (dlcdn.apache.org)|151.101.2.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 299350810 (285M) [application/x-gzip]\n","Saving to: ‘spark-3.3.1-bin-hadoop3.tgz’\n","\n","spark-3.3.1-bin-had 100%[===================>] 285.48M   228MB/s    in 1.3s    \n","\n","2022-12-06 01:48:48 (228 MB/s) - ‘spark-3.3.1-bin-hadoop3.tgz’ saved [299350810/299350810]\n","\n"]}],"source":["!wget https://dlcdn.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n","!tar -zxf /kaggle/working/spark-3.3.1-bin-hadoop3.tgz"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:48:53.731798Z","iopub.status.busy":"2022-12-06T01:48:53.731392Z","iopub.status.idle":"2022-12-06T01:48:53.744548Z","shell.execute_reply":"2022-12-06T01:48:53.743251Z","shell.execute_reply.started":"2022-12-06T01:48:53.731759Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["env: SPARK_HOME=/kaggle/working/spark-3.3.1-bin-hadoop3\n","env: HADOOP_HOME=/kaggle/working/spark-3.3.1-bin-hadoop3\n","env: PATH=/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/kaggle/working/spark-3.3.1-bin-hadoop3/sbin\n"]}],"source":["%env SPARK_HOME = /kaggle/working/spark-3.3.1-bin-hadoop3\n","%env HADOOP_HOME = /kaggle/working/spark-3.3.1-bin-hadoop3\n","%env PATH=/opt/conda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/kaggle/working/spark-3.3.1-bin-hadoop3/sbin"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:49:59.232926Z","iopub.status.busy":"2022-12-06T01:49:59.232450Z","iopub.status.idle":"2022-12-06T01:50:03.215902Z","shell.execute_reply":"2022-12-06T01:50:03.214434Z","shell.execute_reply.started":"2022-12-06T01:49:59.232890Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/working/orchai_validator\n"]}],"source":["is_kaggle = True\n","git_access_token = \"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\"\n","git_url = \"https://github.com/hainguyenhuy2002/orchai_validator.git\"\n","\n","if is_kaggle:\n","    from git import Repo\n","    repo = Repo.clone_from(f\"https://{git_access_token}:x-oauth-basic@{git_url.split('//')[1]}\", './orchai_validator')\n","    repo.git.checkout('topic/upload')\n","\n","    %cd orchai_validator\n","else:\n","    %cd .."]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:51:11.712030Z","iopub.status.busy":"2022-12-06T01:51:11.711497Z","iopub.status.idle":"2022-12-06T01:52:05.285990Z","shell.execute_reply":"2022-12-06T01:52:05.284198Z","shell.execute_reply.started":"2022-12-06T01:51:11.711987Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hCollecting omegaconf\n","  Downloading omegaconf-2.2.3-py3-none-any.whl (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: PyYAML>=5.1.0 in /opt/conda/lib/python3.7/site-packages (from omegaconf->-r requirements.txt (line 2)) (6.0)\n","Collecting antlr4-python3-runtime==4.9.*\n","  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hBuilding wheels for collected packages: pyspark, antlr4-python3-runtime\n","  Building wheel for pyspark (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845513 sha256=980c75df4079185c160550c0ccce05c439f5e2c68229cc6b836e61b8792c3134\n","  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144575 sha256=434cd17ccd41fd140e0c4da6783954b1115531ab3fedbfb3f26ee540174da27b\n","  Stored in directory: /root/.cache/pip/wheels/8b/8d/53/2af8772d9aec614e3fc65e53d4a993ad73c61daa8bbd85a873\n","Successfully built pyspark antlr4-python3-runtime\n","Installing collected packages: py4j, antlr4-python3-runtime, pyspark, omegaconf\n","  Attempting uninstall: py4j\n","    Found existing installation: py4j 0.10.9.7\n","    Uninstalling py4j-0.10.9.7:\n","      Successfully uninstalled py4j-0.10.9.7\n","Successfully installed antlr4-python3-runtime-4.9.3 omegaconf-2.2.3 py4j-0.10.9.5 pyspark-3.3.1\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip3 install -r requirements.txt"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:53:17.890864Z","iopub.status.busy":"2022-12-06T01:53:17.890353Z","iopub.status.idle":"2022-12-06T01:53:18.046373Z","shell.execute_reply":"2022-12-06T01:53:18.045404Z","shell.execute_reply.started":"2022-12-06T01:53:17.890826Z"},"trusted":true},"outputs":[],"source":["from labeling.etl_processor import ETLProcessor\n","from labeling.utils import get_spark, query, upload\n","from omegaconf import OmegaConf\n","import pyspark.sql.functions as F"]},{"cell_type":"markdown","metadata":{},"source":["## ETL"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:53:19.978539Z","iopub.status.busy":"2022-12-06T01:53:19.977506Z","iopub.status.idle":"2022-12-06T01:53:27.910281Z","shell.execute_reply":"2022-12-06T01:53:27.908906Z","shell.execute_reply.started":"2022-12-06T01:53:19.978482Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["22/12/06 01:53:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]}],"source":["spark = get_spark()\n","\n","def sample(spark):\n","    config = OmegaConf.load(\"./config/etl.yml\")\n","    df = query(spark, **config['from'])\n","    df.printSchema()\n","    max_height = df.agg(F.max(df.block_height)).collect()[0].asDict()['max(block_height)']\n","    df = df.filter(df.block_height < max_height)\n","    return df"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:53:27.913634Z","iopub.status.busy":"2022-12-06T01:53:27.912977Z","iopub.status.idle":"2022-12-06T01:53:40.161750Z","shell.execute_reply":"2022-12-06T01:53:40.160549Z","shell.execute_reply.started":"2022-12-06T01:53:27.913592Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Successfully queried data from database\n","root\n"," |-- block_height: integer (nullable = true)\n"," |-- operator_address: string (nullable = true)\n"," |-- jailed: boolean (nullable = true)\n"," |-- status: string (nullable = true)\n"," |-- tokens: decimal(38,0) (nullable = true)\n"," |-- commission_rate: float (nullable = true)\n"," |-- delegator_shares: decimal(38,0) (nullable = true)\n"," |-- self_bonded: decimal(38,0) (nullable = true)\n"," |-- propose: boolean (nullable = true)\n"," |-- vote: boolean (nullable = true)\n","\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["df = sample(spark)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:53:40.164339Z","iopub.status.busy":"2022-12-06T01:53:40.163495Z","iopub.status.idle":"2022-12-06T01:53:43.460326Z","shell.execute_reply":"2022-12-06T01:53:43.459342Z","shell.execute_reply.started":"2022-12-06T01:53:40.164291Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["------------------------------------------------\n","Successfully converted voting_power_score column\n","------------------------------------------------\n","------------------------------------------------\n","Successfully converted commission_score column\n","------------------------------------------------\n","------------------------------------------------\n","Successfully converted self_bonded_score column\n","------------------------------------------------\n","------------------------------------------------\n","Successfully converted vote_propose_score column\n","------------------------------------------------\n","------------------------------------------------\n","Sucessfully converted final_score\n","------------------------------------------------\n"]}],"source":["accept_rate = 0.1\n","concentration_level = 0.9\n","vote_score = 2\n","propose_score = 6\n","A = 9\n","B = 4\n","C = 2\n","D = 4\n","\n","df = ETLProcessor.data_scoring(df, accept_rate, concentration_level, vote_score, propose_score, A, B, C, D)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# df.write.csv(\"./data/orchai_etl\", header=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Upload"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-12-06T01:53:54.992906Z","iopub.status.busy":"2022-12-06T01:53:54.992526Z","iopub.status.idle":"2022-12-06T01:53:55.067252Z","shell.execute_reply":"2022-12-06T01:53:55.065647Z","shell.execute_reply.started":"2022-12-06T01:53:54.992875Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'host': '127.0.0.1', 'port': 5432, 'user': 'postgres', 'password': 'dbadmin', 'database': 'orchai', 'table': 'validator_score', 'batchsize': 1000}\n"]}],"source":["df = df.drop('jailed', 'status', 'tokens', 'commission_rate', 'delegator_shares', 'self_bonded', 'propose', 'vote')\n","config = OmegaConf.load(\"./config/etl.yml\")\n","print(config.to)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["upload(df, **config.to)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"953727d1e95b678a3946b8bfc3d5a175ea50e205ad779370d1fb67fabbb85d7d"}}},"nbformat":4,"nbformat_minor":4}
